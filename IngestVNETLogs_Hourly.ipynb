{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "### Notebook ingests exploded VNET logs into ADLS & Kusto by listing files recursively at source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "from notebookutils import mssparkutils\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run ADFJobs/CommonUtilFunctions/LinkedService_UtilFunctions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "slice_date = '2024-06-26 02'\n",
        "slice_start_date = '2024-06-17 15:00:00'\n",
        "slice_end_date = '2024-06-18 15:00:00'\n",
        "SOURCE_BLOB_CONTAINER_NAME = \"insights-logs-flowlogflowevent\"\n",
        "SOURCE_BLOB_ACCOUNT_NAME = \"vnetflowstorageeus\" \n",
        "SOURCE_LINKED_SERVICE_NAME = \"AzureDataLakeStorage_vnetflowstorageeus2_LinkedService\"\n",
        "TARGET_ADLS_CONTAINER_NAME = \"prod\"\n",
        "TARGET_ADLS_ACCOUNT_NAME = \"testadlsgen20\"\n",
        "TARGET_ADLS_LINKED_SERVICE_NAME = \"AzureDataLakeStorage_testadlsgen20_LinkedService\"\n",
        "TARGET_KUSTO_LINKED_SERVICE_NAME = \"AzureDataExplorer_Sipvnetlogsprod_LinkedService\"\n",
        "TARGET_KUSTO_DATABASE_NAME = \"vnetlogs\"\n",
        "TARGET_KUSTO_TABLE_NAME = \"logs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# SOURCE_BLOB_ACCOUNT_NAME = \"vnetflowstoragewus2\"\n",
        "# SOURCE_BLOB_ACCOUNT_NAME = \"vnetflowstorageeus\"\n",
        "# SOURCE_BLOB_ACCOUNT_NAME = \"vnetflowstoragecus\"\n",
        "# SOURCE_BLOB_ACCOUNT_NAME = \"vnetflowstoragescus\"\n",
        "# TARGET_ADLS_ACCOUNT_NAME = \"testadlsgen20\"\n",
        "\n",
        "#TARGET_ADLS_LINKED_SERVICE_NAME = \"AzureDataLakeStorage_testadlsgen20_LinkedService\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Source Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\n",
        "source_full_account_name = f\"{SOURCE_BLOB_ACCOUNT_NAME}.dfs.core.windows.net\"\n",
        "spark.conf.set(f\"spark.storage.synapse.{source_full_account_name}.linkedServiceName\", SOURCE_LINKED_SERVICE_NAME)\n",
        "SOURCE_RELATIVE_PATH_TEMPLATE = \"flowLogResourceID=/\"\n",
        "source_url = f\"abfss://{SOURCE_BLOB_CONTAINER_NAME}@{source_full_account_name}/{SOURCE_RELATIVE_PATH_TEMPLATE}\"\n",
        "print(source_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Listing source files recursively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "fl_res_ids = mssparkutils.fs.ls(source_url)\n",
        "\n",
        "hour_file_list = []\n",
        "slice_time = datetime.strptime(slice_date, '%Y-%m-%d %H')\n",
        "file_name_append_text = '/y=%s/m=%s/d=%s/h=%s/m=00'% (\n",
        "    datetime.strftime(slice_time, '%Y'),\n",
        "    datetime.strftime(slice_time, '%m'),\n",
        "    datetime.strftime(slice_time, '%d'),\n",
        "    datetime.strftime(slice_time, '%H')\n",
        ")\n",
        "fl_res_ids = mssparkutils.fs.ls(source_url)\n",
        "for res_id in fl_res_ids:\n",
        "    vnets_list = mssparkutils.fs.ls(res_id.path)\n",
        "    for vnet in vnets_list:\n",
        "        file_hour_url = vnet.path + file_name_append_text\n",
        "        hour_file_list.append(file_hour_url)\n",
        "\n",
        "file_list = []\n",
        "ex_count = 0\n",
        "mac_count = 0\n",
        "file_count = 0\n",
        "for hour in hour_file_list:\n",
        "    try:\n",
        "        macs_list = mssparkutils.fs.ls(hour)\n",
        "        mac_count += 1\n",
        "        for mac in macs_list:            \n",
        "            mac_full_url = mac.path + \"/PT1H.json\"           \n",
        "            file_list.append(mac_full_url)\n",
        "            file_count += 1\n",
        "    except Exception as e:\n",
        "        # print(\"Exception occure. Ignoring location : \"+hour)\n",
        "        ex_count += 1\n",
        "\n",
        "print(\"Count of paths not found: %s found: %s file_count: %s\" % (int(ex_count), int(mac_count), int(file_count))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Source schema definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Defining the schema\n",
        "schema = StructType([\n",
        "    StructField(\"records\", \n",
        "        ArrayType(StructType([\n",
        "            StructField(\"category\", StringType(),True),\n",
        "            StructField(\"flowLogGUID\", StringType(),True),\n",
        "            StructField(\"flowLogResourceID\", StringType(),True),\n",
        "            StructField(\"flowLogVersion\", LongType(),True),\n",
        "            StructField(\"flowRecords\", \n",
        "                StructType([\n",
        "                    StructField(\"flows\", \n",
        "                        ArrayType(StructType\n",
        "                        ([\n",
        "                        StructField(\"aclID\",StringType(),True),\n",
        "                        StructField(\"flowGroups\", \n",
        "                            ArrayType(StructType([\n",
        "                            StructField(\"flowTuples\", ArrayType(StringType(),True)),\n",
        "                            StructField(\"rule\",StringType(),True)])),True)\n",
        "                        ])),True)]),True),\n",
        "            StructField(\"macAddress\", StringType(),True),\n",
        "            StructField(\"operationName\", StringType(),True),\n",
        "            StructField(\"targetResourceID\", StringType(),True),\n",
        "            StructField(\"time\", StringType(),True)])),True)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read and Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Time to read an entire hour\n",
        "df_hour = spark.read.schema(schema).option(\"multiline\",\"true\").json(file_list).withColumn(\"input_file_name\", input_file_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Splitting df_hour to records, repartition for proper distribution\n",
        "df_records = df_hour.withColumn(\"records\", explode(\"records\"))\n",
        "df_records = df_records.repartition(160)\n",
        "df_records.persist()\n",
        "print(df_records.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Display distribution across partitions\n",
        "# rcrd_cnt_part = df_records.withColumn(\"partitionid\",spark_partition_id()).groupBy(\"partitionid\").count()\n",
        "# print(rcrd_cnt_part.show(160))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creating df_exploded\n",
        "df_exploded = (df_records\n",
        "               .withColumn(\"category\",col(\"records.category\"))\n",
        "               .withColumn(\"flowLogGUID\",col(\"records.flowLogGUID\"))\n",
        "               .withColumn(\"flowLogResourceID\",col(\"records.flowLogResourceID\"))\n",
        "               .withColumn(\"flowLogVersion\",col(\"records.flowLogVersion\"))\n",
        "               .withColumn(\"macAddress\",col(\"records.macAddress\"))\n",
        "               .withColumn(\"operationName\",col(\"records.operationName\"))\n",
        "               .withColumn(\"targetResourceID\",col(\"records.targetResourceID\"))\n",
        "               .withColumn(\"flowlog_collected_time\",col(\"records.time\"))\n",
        "               .withColumn(\"flowRecords\",explode(col(\"records.flowRecords.flows\")))\n",
        "               .withColumn(\"aclID\",col(\"flowRecords.aclID\"))\n",
        "               .withColumn(\"flowGroups\",explode(col(\"flowRecords.flowGroups\")))\n",
        "               .withColumn(\"flowTuples\",explode(col(\"flowGroups.flowTuples\")))               \n",
        "               .withColumn(\"rule\",col(\"flowGroups.rule\"))).select(\"category\",\n",
        "                \"flowLogGUID\",\n",
        "                \"flowLogResourceID\",\n",
        "                \"flowLogVersion\",\n",
        "                \"aclID\",\n",
        "                \"flowGroups\",\n",
        "                \"rule\",\n",
        "                \"macAddress\",\n",
        "                \"operationName\",\n",
        "                \"targetResourceID\",\n",
        "                \"flowlog_collected_time\",\n",
        "                \"input_file_name\",\n",
        "                \"flowTuples\")\n",
        "               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creating df_exploded_full\n",
        "df_exploded_full = (df_exploded.withColumn(\"flowTupleslist\", split(df_exploded.flowTuples,\",\"))\n",
        "                            .withColumn(\"ETL_DATE\", date_format(lit(slice_date), 'yyyy-MM-dd'))\n",
        "                            .withColumn(\"Region\",regexp_replace(lit(SOURCE_BLOB_ACCOUNT_NAME),\"vnetflowstorage\",\"\"))\n",
        "                            .withColumn(\"FLOWLOG_WINDOW_START_TIME\", date_format(lit(slice_start_date), 'yyyy-MM-dd HH:00:00'))\n",
        "                            .withColumn(\"FLOWLOG_WINDOW_END_TIME\", date_format(lit(slice_end_date), 'yyyy-MM-dd HH:00:00')))\n",
        "df_exploded_full = (df_exploded_full.select(\"category\",\n",
        "                            \"flowLogGUID\",\n",
        "                            \"flowLogResourceID\",\n",
        "                            \"flowLogVersion\",\n",
        "                            \"aclID\",\n",
        "                            \"macAddress\",\n",
        "                            \"operationName\",\n",
        "                            \"targetResourceID\",\n",
        "                            col(\"flowlog_collected_time\").cast(\"timestamp\"),\n",
        "                            \"flowTuples\",\n",
        "                            \"rule\",\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(0).alias('Timestamp'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(1).alias('Source_IP'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(2).alias('Destination_IP'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(3).alias('Source_Port'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(4).alias('Destination_Port'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(5).alias('Protocol'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(6).alias('TrafficFlow'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(7).alias('Flow_State'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(8).alias('Flow_encryption_status'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(9).cast(\"long\").alias('Packets_Source_to_Destination'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(10).cast(\"long\").alias('Bytes_Source_to_Destination'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(11).cast(\"long\").alias('Packets_Destination_to_Source'),\n",
        "                            df_exploded_full.flowTupleslist.__getitem__(12).cast(\"long\").alias('Bytes_Destination_to_Source'),\n",
        "                            \"input_file_name\",\n",
        "                            \"Region\",\n",
        "                            \"ETL_DATE\",\n",
        "                            \"FLOWLOG_WINDOW_START_TIME\",\n",
        "                            \"FLOWLOG_WINDOW_END_TIME\")\n",
        "                        )\n",
        "df_exploded_full.persist()\n",
        "print(df_exploded_full.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write to ADLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#write to adls location\n",
        "TARGET_ADLS_PATH = \"feeds/logs\"\n",
        "\n",
        "target_sink_url = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (\n",
        "    TARGET_ADLS_CONTAINER_NAME,\n",
        "    TARGET_ADLS_ACCOUNT_NAME,\n",
        "    TARGET_ADLS_PATH\n",
        ")\n",
        "configure_account_token(\n",
        "    f'{TARGET_ADLS_ACCOUNT_NAME}.dfs.core.windows.net',\n",
        "    TARGET_ADLS_LINKED_SERVICE_NAME\n",
        ")\n",
        "\n",
        "print(target_sink_url)\n",
        "df_exploded_full.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").partitionBy(\"ETL_DATE\",\"Region\").save(target_sink_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(target_sink_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write to Kusto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#write to kusto table\n",
        "(df_exploded_full\n",
        "        .write\n",
        "\t    .format(\"com.microsoft.kusto.spark.synapse.datasource\")\n",
        "\t    .option(\"spark.synapse.linkedService\", TARGET_KUSTO_LINKED_SERVICE_NAME)\n",
        "\t    .option(\"kustoDatabase\", TARGET_KUSTO_DATABASE_NAME)\n",
        "\t    .option(\"kustoTable\", TARGET_KUSTO_TABLE_NAME)\n",
        "\t    .mode(\"Append\")\n",
        "        .save()\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
