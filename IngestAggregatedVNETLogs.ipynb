{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "session_starting",
              "parent_msg_id": "f3edb433-ab91-46fa-9b56-988856ae90e3",
              "queued_time": "2024-10-09T18:55:46.4216895Z",
              "session_id": null,
              "session_start_time": "2024-10-09T18:55:46.4673541Z",
              "spark_jobs": null,
              "spark_pool": null,
              "state": "session_starting",
              "statement_id": -1,
              "statement_ids": []
            },
            "text/plain": [
              "StatementMeta(, , -1, SessionStarting, , SessionStarting)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Notebook ingests exploded VNET logs into ADLS & Kusto by listing files recursively at source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "waiting",
              "parent_msg_id": "70178e5e-3582-4568-84fd-cb602a372b22",
              "queued_time": "2024-10-09T18:56:05.9730334Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null,
              "statement_ids": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, , Waiting)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "from notebookutils import mssparkutils\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run ADFJobs/CommonUtilFunctions/LinkedService_UtilFunctions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "waiting",
              "parent_msg_id": "3c2fe035-ac10-4254-b91e-692aae67eb60",
              "queued_time": "2024-10-09T18:56:26.2235623Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "waiting",
              "statement_id": null,
              "statement_ids": null
            },
            "text/plain": [
              "StatementMeta(, , , Waiting, , Waiting)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "WRITE = True  # Controls Writes to Kusto Database, useful to test functionality without writing data\n",
        "DEBUG = True  # Prints debug statements to output, useful for debugging\n",
        " \n",
        "ETL_DATE = \"yesterday\"  # today, yesterday or year-month-date string, controls what date will be pulled from ADLS\n",
        "REGION = \"wcus\"  # Currently only support one region\n",
        "CONTAINER = \"prod\"  # Currently only supporting prod container\n",
        "DATA_PATH = \"feeds/logs\"  # Directory path in ADLS container\n",
        "DOMAIN = \"sipvnetlogsadlsgen2.dfs.core.windows.net\"  # ADLS domain service location\n",
        "#DOMAIN = \"testadlsgen20.dfs.core.windows.net\"  # ADLS domain service location\n",
        "#LINKED_SERVICE_NAME = \"sipvnetlogsadls_connector\"  # Synapse Managed Identity Linked Service Name for ADLS domain\n",
        "LINKED_SERVICE_NAME = \"AzureDataLakeStorage_sipvnetlogsadlsgen2_LinkedService\"\n",
        "SIP_DATABASE = \"vnetlogs_uat\"  # Kusto Database name used to store data from ADLS\n",
        "SIP_TABLE = \"logs_aggregated\"  # Kusto table name\n",
        "#SIP_LINKED_SERVICE_NAME = \"VNetFlowLogsSynapse_Connector\"  # Synapse Managed Identity for writing to Kusto\n",
        "SIP_LINKED_SERVICE_NAME = \"AzureDataExplorer_Sipvnetlogsprod_LinkedService\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Source Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta, timezone\n",
        "import re\n",
        " \n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import functions\n",
        "from pyspark.sql.types import StringType\n",
        " \n",
        " \n",
        "RESOURCE_GROUP_PATTERN_STR = r\"^.*\\/resourcegroups\\/([\\w|\\-|\\_]+)\\/.*$\"\n",
        "RESOURCE_GROUP_PATTERN = re.compile(RESOURCE_GROUP_PATTERN_STR, re.I)\n",
        " \n",
        "UUID_PATTERN_STR = (\n",
        "    r\"^.*\\/([a-f0-9]{8}-?[a-f0-9]{4}-?4\"\n",
        "    r\"[a-f0-9]{3}-?[89ab][a-f0-9]{3}-?[a-f0-9]{12})\\/.*?$\"\n",
        ")\n",
        "UUID_PATTERN = re.compile(UUID_PATTERN_STR, re.I)\n",
        " \n",
        " \n",
        "def convert_ts(ts: int) -> datetime:\n",
        "    \"\"\"Helper Function converting unix timestamps to python datetime.\n",
        " \n",
        "    Args:\n",
        "        ts (int): a unix timestamp in milliseconds format.\n",
        " \n",
        "    Returns:\n",
        "        datetime: UTC datetime object of timestamp.\n",
        "    \"\"\"\n",
        "    return datetime.fromtimestamp(ts / 1e3).replace(tzinfo=timezone.utc)\n",
        " \n",
        " \n",
        "def convert_dt(dt: datetime) -> int:\n",
        "    \"\"\"Converts python datetime to unix timestamp in milliseconds.\n",
        " \n",
        "    Args:\n",
        "        dt (datetime): a python datetime object to convert to a timestamp.\n",
        " \n",
        "    Returns:\n",
        "        int: unix timestamp with milliseconds.\n",
        "    \"\"\"\n",
        "    return int(dt.timestamp() * 1000)\n",
        " \n",
        " \n",
        "def log_debug(debug_str: str, *args):\n",
        "    \"\"\"Helper Function for printing debug statements.\n",
        " \n",
        "    Args:\n",
        "        debug_str (str): Python string with interpolation embedded.\n",
        "        args (any): interpolation args for debug str.\n",
        "    \"\"\"\n",
        "    if DEBUG:\n",
        "        values = [a() if callable(a) else a for a in args]\n",
        "        print(debug_str.format(*values))\n",
        " \n",
        " \n",
        "class ADLSConnector:\n",
        " \n",
        "    def __init__(\n",
        "        self,\n",
        "        date_str: str = ETL_DATE,\n",
        "        region: str = REGION,\n",
        "        container: str = CONTAINER,\n",
        "        base_data_path: str = DATA_PATH,\n",
        "        domain: str = DOMAIN,\n",
        "        linked_service_name: str = LINKED_SERVICE_NAME,\n",
        "    ):\n",
        "        \"\"\"Provides connection to Azure DataLake Gen2 as a Linked Service.\n",
        "        Generates hourly timestamps based on date_str provided to query ADLS folder.\n",
        "        Reads the constructed parquet files in ADLS folder and returns pySpark.DataFrame.\n",
        " \n",
        "        Args:\n",
        "            date_str (str, optional): Folder Date string to read data parquet data files. Defaults to ETL_DATE.\n",
        "            region (str, optional): ADLS path based region string. Defaults to PARAMETER:REGION.\n",
        "            container (str, optional): ADLS Container string Defaults to PARAMETER:CONTAINER.\n",
        "            base_data_path (str, optional): ADLS base folder path Defaults to PARAMETER:DATA_PATH.\n",
        "            domain (str, optional): ADLS service domain name. Defaults to PARAMETER:DOMAIN.\n",
        "            linked_service_name (str, optional): Name of ADLS Linked Service Defaults to PARAMETER:LINKED_SERVICE_NAME.\n",
        "        \"\"\"\n",
        "        self._etl_date_str = self._str_to_date(date_str)\n",
        "        self._region = region\n",
        "        self._container = container\n",
        "        self._data_path = base_data_path\n",
        "        self._adls_domain = domain\n",
        "        self._linked_service_name = linked_service_name\n",
        " \n",
        "    def _str_to_date(self, date_str: str) -> str:\n",
        "        \"\"\"Converts Date string to ADLS expected format folder path.\n",
        " \n",
        "        Args:\n",
        "            date_str (str): Some date string to be converted.\n",
        " \n",
        "        Returns:\n",
        "            str: date string in MM/DD/YYYY format\n",
        "        \"\"\"\n",
        " \n",
        "        def str_to_dt(date_str: str) -> datetime:\n",
        "            _today = datetime.today().replace(\n",
        "                hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc\n",
        "            )\n",
        "            _yesterday = _today - timedelta(days=1)\n",
        "            try:\n",
        "                return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "                if date_str.lower() == \"today\":\n",
        "                    return _today\n",
        "                elif date_str.lower() == \"yesterday\":\n",
        "                    return _yesterday\n",
        "                else:\n",
        "                    try:\n",
        "                        datetime.strptime(date_str, \"%m/%d/%Y\")\n",
        "                    except ValueError:\n",
        "                        try:\n",
        "                            return datetime.strptime(date_str, \"%m-%d-%Y\")\n",
        "                        except ValueError:\n",
        "                            return _yesterday\n",
        " \n",
        "        return str_to_dt(date_str).strftime(\"%Y-%m-%d\")\n",
        " \n",
        "    @property\n",
        "    def data_path(self) -> str:\n",
        "        return \"/\".join(\n",
        "            (\n",
        "                self._data_path,\n",
        "                self.etl_date,\n",
        "                self.region,\n",
        "            )\n",
        "        )\n",
        " \n",
        "    @property\n",
        "    def domain(self) -> str:\n",
        "        return self._adls_domain\n",
        " \n",
        "    @property\n",
        "    def linked_service_name(self) -> str:\n",
        "        return self._linked_service_name\n",
        " \n",
        "    @property\n",
        "    def container(self) -> str:\n",
        "        return self._container\n",
        " \n",
        "    @property\n",
        "    def region(self) -> str:\n",
        "        return f\"Region={self._region}\"\n",
        " \n",
        "    @property\n",
        "    def etl_date(self) -> str:\n",
        "        return f\"ETL_DATE={self._etl_date_str}\"\n",
        " \n",
        "    @property\n",
        "    def abfss_domain(self) -> str:\n",
        "        return f\"{self.container}@{self.domain}\"\n",
        " \n",
        "    @property\n",
        "    def parquet_read_path(self) -> str:\n",
        "        return f\"abfss://{self.abfss_domain}/{self.data_path}/\"\n",
        " \n",
        "    @property\n",
        "    def abfss_tmp_path(self) -> str:\n",
        "        return f\"abfss://{self.abfss_domain}/tmpStreamingDir/\"\n",
        " \n",
        "    def get_hourly_timestamps(\n",
        "        self, first_hour_ts: int, last_hour_ts: int\n",
        "    ) -> list:\n",
        "        \"\"\"Returns a list of millisecond precision timestamps\n",
        "        from the currently selected ETL_DATE.\n",
        "        \"\"\"\n",
        " \n",
        "        first_dt = convert_ts(first_hour_ts)\n",
        "        last_dt = convert_ts(last_hour_ts)\n",
        " \n",
        "        #start_time = first_dt.replace(minute=0, second=0, microsecond=0)\n",
        "        #last_dt = last_dt.replace(minute=0, second=0, microsecond=0)\n",
        "\n",
        "        # start_time = first_dt.replace(minute=0, second=0, microsecond=0)\n",
        "        last_dt = last_dt.replace(minute=0, second=0, microsecond=0)\n",
        "        \"\"\"Please specify the time window\"\"\"\n",
        "        start_time = last_dt - timedelta(hours=1)\n",
        "\n",
        " \n",
        "        hourly_ts = [convert_dt(start_time)]\n",
        "        while start_time < last_dt:\n",
        "            start_time += timedelta(hours=1)\n",
        "            hourly_ts.append(convert_dt(start_time))\n",
        " \n",
        "        return hourly_ts\n",
        " \n",
        "    @staticmethod\n",
        "    def get_min_ts(df: DataFrame) -> int:\n",
        "        return int(df.agg(functions.min(df.Timestamp)).head(1)[0][0])\n",
        " \n",
        "    @staticmethod\n",
        "    def get_max_ts(df: DataFrame) -> int:\n",
        "        return int(df.agg(functions.max(df.Timestamp)).head(1)[0][0])\n",
        " \n",
        "    def query(self) -> DataFrame:\n",
        "        \"\"\"Reads from parquet files from ADLS storage path.\n",
        " \n",
        "        Returns:\n",
        "            DataFrame: pySpark.DataFrame rows read from storage path.\n",
        "        \"\"\"\n",
        "        spark.conf.set(\n",
        "            \"spark.storage.synapse.linkedServiceName\", self.linked_service_name\n",
        "        )\n",
        "        spark.conf.set(\n",
        "            \"spark.storage.synapse.teststorage.dfs.core.windows.net.linkedServiceName\",\n",
        "            self.linked_service_name,\n",
        "        )\n",
        "        spark.conf.set(\n",
        "            \"spark.hadoop.fs.azure.account.oauth.provider.type.teststorage.dfs.core.windows.net\",\n",
        "            \"microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\",\n",
        "        )\n",
        " \n",
        "        log_debug(self.parquet_read_path)\n",
        "        return spark.read.parquet(self.parquet_read_path)\n",
        " \n",
        "    def write(self, df: DataFrame, hourly_path: str) -> None:\n",
        "        \"\"\"Writes data to ADLS Gen2 folder.\n",
        " \n",
        "        Args:\n",
        "            df (DataFrame): DataFrame rows to write to file.\n",
        "            hourly_path (str): hourly string of data\n",
        " \n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        spark.conf.set(\n",
        "            \"spark.storage.synapse.teststorage.dfs.core.windows.net.linkedServiceName\",\n",
        "            self._linked_service_name,\n",
        "        )\n",
        "        spark.conf.set(\n",
        "            \"spark.hadoop.fs.azure.account.oauth.provider.type.teststorage.dfs.core.windows.net\",\n",
        "            \"microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\",\n",
        "        )\n",
        "        write_path = f\"abfss://{self.container}@{self.domain}/{self.data_path}/{hourly_path}/\"\n",
        "        log_debug(write_path)\n",
        "        if WRITE:\n",
        "            return df.write.parquet(write_path)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ADLS Connector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "adls = ADLSConnector()\n",
        "log_debug(\"Querying for {}\", adls.etl_date)\n",
        "df = adls.query()\n",
        "log_debug(\"Total Rows [{}] returned\", df.count)\n",
        "log_debug(\"Columns [{}] returned\", df.columns)\n",
        "# Get Unix TimeStamps for the given date in 1 hour increments\n",
        " \n",
        "# Get earliest and latest timestamps in the dataset\n",
        "first_hour_ts = ADLSConnector.get_min_ts(df)\n",
        "last_hour_ts = ADLSConnector.get_max_ts(df)\n",
        "daily_ts = adls.get_hourly_timestamps(first_hour_ts, last_hour_ts)\n",
        "log_debug(\"TimeStamps Created for Hourly Processing [{}]\", daily_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "first_hour_ts = ADLSConnector.get_min_ts(df)\n",
        "last_hour_ts = ADLSConnector.get_max_ts(df)\n",
        "daily_ts = adls.get_hourly_timestamps(first_hour_ts, last_hour_ts)\n",
        "log_debug(\"TimeStamps Created for Hourly Processing [{}]\", daily_ts)\n",
        "\n",
        "#print(first_hour_ts)\n",
        "#print(last_hour_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class SIPWriter:\n",
        "    def __init__(self,\n",
        "                 database: str = SIP_DATABASE,\n",
        "                 table: str = SIP_TABLE,\n",
        "                 region: str = REGION,\n",
        "                 linked_service_name: str = SIP_LINKED_SERVICE_NAME):\n",
        "        \"\"\"Creates a connection to Kusto Cluster from Synapse Linked Service.\n",
        "        Summarizes hourly snapshot of pySpark.DataFrame, generates user defined functions from DataFrame.\n",
        "        Writes a pyspark.DataFrame to Kusto table.\n",
        " \n",
        "        Args:\n",
        "            database (str, optional): Kusto Database name. Defaults to PARAMETERS:SIP_DATABASE.\n",
        "            table (str, optional): Kusto Table name. Defaults to PARAMETERS:SIP_TABLE.\n",
        "            region (str, optional): Region data was read from. Defaults to PARAMETERS:REGION.\n",
        "            linked_service_name (str, optional): Synapse Linked Service. Defaults to PARAMETERS:SIP_LINKED_SERVICE_NAME.\n",
        "        \"\"\"\n",
        "        self.spark_conf()\n",
        "        self._database = database\n",
        "        self._table = table\n",
        "        self._region = region\n",
        "        self._linked_service_name = linked_service_name\n",
        " \n",
        "    @property\n",
        "    def database(self) -> str:\n",
        "        return self._database\n",
        "   \n",
        "    @property\n",
        "    def region(self) -> str:\n",
        "        return self._region\n",
        " \n",
        "    @property\n",
        "    def table(self) -> str:\n",
        "        return self._table\n",
        " \n",
        "    @property\n",
        "    def linked_service_name(self) -> str:\n",
        "        return self._linked_service_name\n",
        " \n",
        "    @staticmethod\n",
        "    def summarize(df: DataFrame) -> DataFrame:\n",
        "        \"\"\"Summarizes DataFrame\n",
        " \n",
        "        Args:\n",
        "            df (DataFrame): DataFrame hourly summarization of data.\n",
        " \n",
        "        Returns:\n",
        "            DataFrame: _description_\n",
        "        \"\"\"\n",
        "        return df.groupBy([df.Source_IP, df.Destination_IP, df.TrafficFlow,\n",
        "                            df.Flow_State, df.macAddress, df.flowLogResourceID,\n",
        "                            df.targetResourceID, df.Source_Port, df.Destination_Port]).count()\n",
        " \n",
        "    def resource_groups_and_summarized_dt(self, df: DataFrame, dt: datetime) -> DataFrame:\n",
        "        \"\"\"\n",
        "            Extract Subscription IDs and Target Resource Groups\n",
        "            (flow Resource Group is always NetworkWatcher.).\n",
        "            Adds summarized_dt date column, region str, and changes port columns to int.\n",
        " \n",
        "        Args:\n",
        "            df (DataFrame): pyspark.DataFrame rows\n",
        "            dt (datetime): hourly timestamp from dataframe\n",
        " \n",
        "        Returns:\n",
        "            DataFrame: pyspark.DataFrame with user defined functions.\n",
        "        \"\"\"\n",
        "        def match_subscription_fn(resource_str: str) -> str:\n",
        "            _match = UUID_PATTERN.match(resource_str)\n",
        "            if (_match):\n",
        "                return _match.groups(1)[0]\n",
        "            return \"\"\n",
        " \n",
        "        def match_resource_group_fn(resource_str: str) -> str:\n",
        "            _match = RESOURCE_GROUP_PATTERN.match(resource_str)\n",
        "            if (_match):\n",
        "                return _match.groups(1)[0]\n",
        "            return \"\"\n",
        " \n",
        "        match_subscription_udf = functions.udf(match_subscription_fn, StringType())\n",
        "        match_resource_group_udf = functions.udf(match_resource_group_fn, StringType())\n",
        " \n",
        "        return df.withColumn('flowLogSubscriptionID', match_subscription_udf(df.flowLogResourceID)) \\\n",
        "                 .withColumn(\"targetSubscriptionID\", match_subscription_udf(df.targetResourceID)) \\\n",
        "                 .withColumn(\"targetResourceGroup\", match_resource_group_udf(df.targetResourceID)) \\\n",
        "                 .withColumn(\"Source_Port\", df[\"Source_Port\"].cast('int')) \\\n",
        "                 .withColumn(\"Destination_Port\", df[\"Destination_Port\"].cast('int')) \\\n",
        "                 .withColumn('summarizedDT', functions.lit(dt)) \\\n",
        "                 .withColumn('region', functions.lit(self.region))\n",
        " \n",
        "    @classmethod\n",
        "    def create_table_column_map(cls, df: DataFrame) -> str:\n",
        "        \"\"\"\n",
        "           Class Method for creating column Map for writing to Table.\n",
        "        Args:\n",
        "            df (DataFrame): pyspark.DataFrame rows\n",
        " \n",
        "        Returns:\n",
        "            str: returns csv column mappings.\n",
        "        \"\"\"\n",
        "        cvsMap = \"[\"\n",
        "        i = 0\n",
        "        for col in df.columns:\n",
        "            col_type = df.schema[col].dataType\n",
        "            cvsMap += f\"\"\"{{\"Name\":\"{col}\",\"{col_type}\":{i}}},\"\"\"\n",
        "            i+=1\n",
        "        cvsMap = cvsMap.rstrip(\",\")\n",
        "        cvsMap += \"]\"\n",
        "        return cvsMap\n",
        " \n",
        "    def spark_conf(self) -> None:\n",
        "        \"\"\"Sets global spark configuration for LinkedService Authorization.\n",
        "        \"\"\"\n",
        "        spark.conf.set('fs.adl.oauth2.access.token.provider.type', 'ClientCredential')\n",
        "        spark.conf.set('fs.adl.oauth2.refresh.url',\n",
        "                       'https://login.microsoftonline.com/72f988bf-86f1-41af-91ab-2d7cd011db47/oauth2/token')\n",
        "        spark.conf.set('spark.synapse.linkedService',\n",
        "                       'VNETFlowLogsSynapse_Connector')\n",
        "        spark.conf.set('fs.azure.account.oauth.provider.type',\n",
        "                       'com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider')\n",
        "        spark.conf.set('spark.sql.streaming.checkpointLocation', '/localWriteCheckpointFolder')\n",
        "        spark.conf.set('spark.sql.codegen.wholeStage', 'false')\n",
        "   \n",
        "    def write(self, df: DataFrame) -> None:\n",
        "        \"\"\"Writes DataFrame rows to Kusto table.\n",
        " \n",
        "        Args:\n",
        "            df (DataFrame): DataFrame rows to write to table.\n",
        " \n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        # Write data to a Kusto table\n",
        "        log_debug(\"Writing {}.{}\", self.database, self.table)\n",
        "        if WRITE:\n",
        "           return df.write. \\\n",
        "                format('com.microsoft.kusto.spark.synapse.datasource'). \\\n",
        "                option('spark.synapse.linkedService', self.linked_service_name). \\\n",
        "                option('kustoDatabase', self.database). \\\n",
        "                option('kustoTable', self.table). \\\n",
        "                option('authType', 'LS'). \\\n",
        "                option('tableCreateOptions', 'CreateIfNotExist'). \\\n",
        "                mode('Append').save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "log_debug(\"# Processing Hourly Aggregations\")\n",
        "sip_writer = SIPWriter()\n",
        "total_summarized_rows = 0\n",
        " \n",
        "for i in range(1, len(daily_ts) - 1): # Loop thru each hour of the day to aggregate data\n",
        "    hourly_snapshot = df.filter((df.Timestamp >= daily_ts[i]) & (df.Timestamp < daily_ts[i+1]))\n",
        "    log_debug(\"- From [{}] to [{}]\\n - Rows [{}]\",\n",
        "              convert_ts(daily_ts[i]),\n",
        "              convert_ts(daily_ts[i+1]),\n",
        "              hourly_snapshot.count)\n",
        "    hourly_snapshot_summary = sip_writer.summarize(hourly_snapshot)\n",
        "    # Add resource columns, summarized_dt and change port columns to int\n",
        "    ext_hourly_summary = sip_writer.resource_groups_and_summarized_dt(hourly_snapshot_summary,\n",
        "                                                                      convert_ts(daily_ts[i]))\n",
        "    log_debug(\"- Extended Columns [{}]\", ext_hourly_summary.columns)\n",
        "    if DEBUG:\n",
        "        total_summarized_rows += ext_hourly_summary.count()\n",
        " \n",
        "    sip_writer.write(ext_hourly_summary)\n",
        "    log_debug(\"=\" * 88)\n",
        " \n",
        "log_debug(\"# Hourlying Aggregation Finished with rows [{}] appended to [{}.{}]\",\n",
        "          total_summarized_rows, sip_writer.database, sip_writer.table)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
